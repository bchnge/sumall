{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_test = '../data/test_insult.csv'\n",
      "f_train = '../data/train_insult.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = pd.read_csv(f_test)\n",
      "df_train = pd.read_csv(f_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 341
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try a bunch of different vectorizers on Comment data\n",
      "\n",
      "#vec = CountVectorizer(stop_words = 'english', ngram_range=(2,2))\n",
      "#vec = CountVectorizer(stop_words = 'english', ngram_range=(1,1))\n",
      "vec = CountVectorizer(binary=False) # better when we don't remove stop words\n",
      "#vec = CountVectorizer(ngram_range=(2,2)) # n >1 seems to decrease performance\n",
      "#vec = CountVectorizer(binary=True)\n",
      "X = vec.fit_transform(list(df_train.Comment))\n",
      "X_test_data = vec.transform(list(df_test.Comment))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 567
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment with a few classifiers\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.naive_bayes import BernoulliNB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 422
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Helper function for evaluating performance of single classifier\n",
      "\n",
      "def get_cv_score(clf, y, X):\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "    ''' wrapper for evaluating classifier. \n",
      "    Provides confusion matrix and accuracy results on 5-fold cross-validation'''\n",
      "    from sklearn.cross_validation import KFold\n",
      "    from sklearn.metrics import accuracy_score\n",
      "    cv_folds = KFold(len(y), n_folds = 5, random_state=123)\n",
      "    results = []\n",
      "    for train_index, test_index in cv_folds:\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        y_train, y_test = y[train_index], y[test_index]\n",
      "        clf.fit(X_train, y_train)\n",
      "        predicted = clf.predict(X_test)\n",
      "        acc = accuracy_score(predicted, y_test)\n",
      "        print acc\n",
      "        print confusion_matrix(y_test, predicted)\n",
      "        results.append(acc)                                      \n",
      "    return np.mean(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 454
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A few other feature extractors based on comments\n",
      "\n",
      "def get_case_ratio(comment):\n",
      "    '''ratio of upper-case characters'''\n",
      "    num_upper = 0\n",
      "    num_lower = 0\n",
      "    num_other = 0\n",
      "    \n",
      "    for character in comment:\n",
      "        if character is not ' ':\n",
      "            if character.isupper():\n",
      "                num_upper += 1\n",
      "            elif character.islower():\n",
      "                num_lower += 1\n",
      "            else:\n",
      "                num_other += 1\n",
      "    if num_upper + num_lower != 0:\n",
      "        return float(num_upper)/(num_upper + num_lower)\n",
      "    else:\n",
      "        return 0\n",
      "    \n",
      "def get_other_ratio(comment):\n",
      "    '''ratio of other characters'''\n",
      "    num_upper = 0\n",
      "    num_lower = 0\n",
      "    num_other = 0\n",
      "    \n",
      "    for character in comment:\n",
      "        if character is not ' ':\n",
      "            if character.isupper():\n",
      "                num_upper += 1\n",
      "            elif character.islower():\n",
      "                num_lower += 1\n",
      "            else:\n",
      "                num_other += 1\n",
      "    if num_upper + num_lower != 0:\n",
      "        return float(num_other)/(num_upper + num_lower)\n",
      "    else:\n",
      "        return 0\n",
      "    \n",
      "def get_num_words(comment):\n",
      "    '''number of words in comment'''\n",
      "    return len(comment.split(' '))\n",
      "\n",
      "def get_comment_length(comment):\n",
      "    '''character length of comment'''\n",
      "    return len(comment)\n",
      "\n",
      "import re\n",
      "def get_num_excl(comment):\n",
      "    '''number of exclamation points'''\n",
      "    res = re.findall('!', comment)\n",
      "    return len(res)\n",
      "\n",
      "import datetime\n",
      "def get_date_info(date, info = 'y'):\n",
      "    '''get date features. y -> year, m -> month, h-> hour, mm-> minute, wd -> day of week'''\n",
      "    if type(date) == float:\n",
      "        result = None\n",
      "    else:\n",
      "        if info == 'y':\n",
      "            result = int(date[0:4])\n",
      "        elif info == 'm':\n",
      "            result = int(date[4:6])\n",
      "        elif info == 'd':\n",
      "            result = int(date[6:8])\n",
      "        elif info == 'h':\n",
      "            result = int(date[8:10])\n",
      "        elif info == 'mm':\n",
      "            result = int(date[10:12])\n",
      "        elif info == 'wd':\n",
      "            result = datetime.datetime.strptime(date[4:6]+'-'+date[6:8] + '-' + date[0:4],'%m-%d-%Y').strftime('%A')\n",
      "        else:\n",
      "            result = None\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 568
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract other features from dataset\n",
      "\n",
      "df_train['ratio_upper'] = df_train.Comment.apply(get_case_ratio)\n",
      "df_train['ratio_other'] = df_train.Comment.apply(get_other_ratio)\n",
      "df_train['comment_length'] = df_train.Comment.apply(get_comment_length)\n",
      "df_train['num_words'] = df_train.Comment.apply(get_num_words)\n",
      "df_train['num_excl'] = df_train.Comment.apply(get_num_excl)\n",
      "df_train['hour'] = df_train.Date.apply(get_date_info, info = 'h')\n",
      "df_train['weekday'] = df_train.Date.apply(get_date_info, info = 'wd')\n",
      "\n",
      "df_test['ratio_upper'] = df_test.Comment.apply(get_case_ratio)\n",
      "df_test['ratio_other'] = df_test.Comment.apply(get_other_ratio)\n",
      "df_test['comment_length'] = df_test.Comment.apply(get_comment_length)\n",
      "df_test['num_words'] = df_test.Comment.apply(get_num_words)\n",
      "df_test['num_excl'] = df_test.Comment.apply(get_num_excl)\n",
      "df_test['hour'] = df_test.Date.apply(get_date_info, info = 'h')\n",
      "df_test['weekday'] = df_test.Date.apply(get_date_info, info = 'wd')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 569
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Combined training and test datasets\n",
      "X2 =  np.column_stack((X.toarray(),\n",
      "                       df_train.ratio_upper,\n",
      "                       df_train.ratio_other,\n",
      "                       df_train.comment_length,\n",
      "                       df_train.num_words,\n",
      "                       df_train.num_excl,\n",
      "                       pd.get_dummies(df_train.hour).values,\n",
      "                       pd.get_dummies(df_train.weekday).values\n",
      "                       ))\n",
      "\n",
      "X2_test =  np.column_stack((X_test_data.toarray(),\n",
      "                       df_test.ratio_upper,\n",
      "                       df_test.ratio_other,\n",
      "                       df_test.comment_length,\n",
      "                       df_test.num_words,\n",
      "                       df_test.num_excl,\n",
      "                       pd.get_dummies(df_test.hour).values,\n",
      "                       pd.get_dummies(df_test.weekday).values\n",
      "                       ))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 582
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dim. reduction using PCA has not given effective results\n",
      "\n",
      "#X3 = PCA(n_components=100).fit_transform(X2)\n",
      "#get_cv_score(SVC(), df_train.Insult, X3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.753164556962\n",
        "[[589   7]\n",
        " [188   6]]\n",
        "0.735443037975"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[581   0]\n",
        " [209   0]]\n",
        "0.71989860583"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[566   0]\n",
        " [221   2]]\n",
        "0.721166032953"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[568   4]\n",
        " [216   1]]\n",
        "0.740177439797"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[582   1]\n",
        " [204   2]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 518,
       "text": [
        "0.73396993470343808"
       ]
      }
     ],
     "prompt_number": 518
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Logistic regression is effective. However, it does have some trouble predicting true positives (1's)\n",
      "get_cv_score(LogisticRegression(C=2), df_train.Insult, X2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.844303797468\n",
        "[[553  43]\n",
        " [ 80 114]]\n",
        "0.848101265823"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[541  40]\n",
        " [ 80 129]]\n",
        "0.844106463878"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[528  38]\n",
        " [ 85 138]]\n",
        "0.828897338403"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[527  45]\n",
        " [ 90 127]]\n",
        "0.84030418251"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[544  39]\n",
        " [ 87 119]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 570,
       "text": [
        "0.84114260961640286"
       ]
      }
     ],
     "prompt_number": 570
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Overall, Naive Bayes performs worse, although it is more effective in detecting true positives. \n",
      "# Moreover, lowering alpha appears to improve the true positive rate.\n",
      "get_cv_score(BernoulliNB(alpha = 0.55), df_train.Insult, X2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.755696202532\n",
        "[[453 143]\n",
        " [ 50 144]]\n",
        "0.779746835443"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[465 116]\n",
        " [ 58 151]]\n",
        "0.77566539924"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[494  72]\n",
        " [105 118]]\n",
        "0.787072243346"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[471 101]\n",
        " [ 67 150]]\n",
        "0.79721166033"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[493  90]\n",
        " [ 70 136]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 571,
       "text": [
        "0.77907846817795323"
       ]
      }
     ],
     "prompt_number": 571
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SVC was ineffective and slow-- too many features, and experimentation with fewer features/pca did not yield great results\n",
      "get_cv_score(SVC(), df_train.Insult, X2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random forest is not very effective, and is especially bad at detecting true positives\n",
      "get_cv_score(RandomForestClassifier(random_state = 123), df_train.Insult, X2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.80253164557\n",
        "[[588   8]\n",
        " [148  46]]\n",
        "0.773417721519"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[568  13]\n",
        " [166  43]]\n",
        "0.776932826362"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[554  12]\n",
        " [164  59]]\n",
        "0.77566539924"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[564   8]\n",
        " [169  48]]\n",
        "0.789607097592"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[569  14]\n",
        " [152  54]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 572,
       "text": [
        "0.78363093805650474"
       ]
      }
     ],
     "prompt_number": 572
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# some other tweaking\n",
      "from sklearn import grid_search\n",
      "NB_grid = grid_search.GridSearchCV(BernoulliNB(), {'alpha':[0,0.2,0.6,0.8,1], 'binarize':[0,0.2,0.4,0.6,0.8,1]})\n",
      "NB_grid.fit(X2, df_train.Insult)\n",
      "print NB_grid.best_params_\n",
      "\n",
      "LR_grid = grid_search.GridSearchCV(LogisticRegression(), {'penalty':('l1','l2'), 'C':[0.1, 0.2,0.4,0.6,0.8,1, 2,4,6]})\n",
      "LR_grid.fit(X2, df_train.Insult)\n",
      "print LR_grid.best_params_\n",
      "\n",
      "RF_grid = grid_search.GridSearchCV(RandomForestClassifier(random_state = 123),{'n_estimators':[10,50]}) \n",
      "RF_grid.fit(X2, df_train.Insult)\n",
      "print RF_grid.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'binarize': 0, 'alpha': 0.6}\n",
        "{'penalty': 'l1', 'C': 0.8}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'n_estimators': 50}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 576
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "def ensemble_cv(clf1, clf2, clf3, clf4, X, y):\n",
      "    ''' wrapper for evaluating 4 classifiers together using mode voting'''\n",
      "    from sklearn.cross_validation import KFold\n",
      "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "    cv_folds = KFold(len(y), n_folds = 5, random_state=123)\n",
      "    results = []\n",
      "    for train_index, test_index in cv_folds:\n",
      "        X_train, X_test = X[train_index], X[test_index]\n",
      "        y_train, y_test = y[train_index], y[test_index]\n",
      "        clf1.fit(X_train, y_train)\n",
      "        clf2.fit(X_train, y_train)\n",
      "        clf3.fit(X_train, y_train)\n",
      "        clf4.fit(X_train, y_train)\n",
      "        clf1_predicted = clf1.predict(X_test)\n",
      "        clf2_predicted = clf2.predict(X_test)\n",
      "        clf3_predicted = clf3.predict(X_test)\n",
      "        clf4_predicted = clf4.predict(X_test)\n",
      "        outcome = [Counter([w1,x1,y1,z1]).most_common(1)[0][0] for w1,x1,y1,z1 in zip(clf1_predicted, clf2_predicted, clf3_predicted, clf4_predicted)]                 \n",
      "        #print outcome[0:20]\n",
      "        acc = accuracy_score(outcome, y_test)\n",
      "        print acc\n",
      "        print confusion_matrix(y_test, outcome)\n",
      "        results.append(acc)                                      \n",
      "    return np.mean(results)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 581
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# blend of logistic regressions, one with a weight towards true positives, and two naive bayes with different alphas appear to \n",
      "# perform better than a single classifier\n",
      "ensemble_cv(LogisticRegression(C=0.8),\n",
      "            LogisticRegression(class_weight = {0:0.25, 1:0.75}),\n",
      "            BernoulliNB(alpha = 0.25),\n",
      "            BernoulliNB(alpha = 0.6),\n",
      "            X2, df_train.Insult)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.840506329114\n",
        "[[535  61]\n",
        " [ 65 129]]\n",
        "0.845569620253"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[528  53]\n",
        " [ 69 140]]\n",
        "0.836501901141"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[530  36]\n",
        " [ 93 130]]\n",
        "0.842839036755"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[528  44]\n",
        " [ 80 137]]\n",
        "0.844106463878"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[539  44]\n",
        " [ 79 127]]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 605,
       "text": [
        "0.84190467022829729"
       ]
      }
     ],
     "prompt_number": 605
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "def ensemble_fit(clf1, clf2, clf3, clf4, X_train, y_train, X_test):\n",
      "    ''' wrapper for evaluating 4 classifiers together using mode voting and producing fitted outcome'''\n",
      "    clf1.fit(X_train, y_train)\n",
      "    clf2.fit(X_train, y_train)\n",
      "    clf3.fit(X_train, y_train)\n",
      "    clf4.fit(X_train, y_train)\n",
      "    clf1_predicted = clf1.predict(X_test)\n",
      "    clf2_predicted = clf2.predict(X_test)\n",
      "    clf3_predicted = clf3.predict(X_test)\n",
      "    clf4_predicted = clf4.predict(X_test)\n",
      "    outcome = [Counter([w1,x1,y1,z1]).most_common(1)[0][0] for w1,x1,y1,z1 in zip(clf1_predicted, clf2_predicted, clf3_predicted, clf4_predicted)]                 \n",
      "    return outcome"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 606
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict test data using ensemble method\n",
      "label = ensemble_fit(LogisticRegression(C=0.8),\n",
      "            LogisticRegression(class_weight = {0:0.25, 1:0.75}),\n",
      "            BernoulliNB(alpha = 0.25),\n",
      "            BernoulliNB(alpha = 0.6),\n",
      "            X2, df_train.Insult, X2_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 612
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "solution = pd.DataFrame(df_test['Comment'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 613
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "solution['label'] = label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 614
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "solution.label.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 615,
       "text": [
        "0    1561\n",
        "1     439\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 615
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "solution.to_csv('q3_solution.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 618
    }
   ],
   "metadata": {}
  }
 ]
}